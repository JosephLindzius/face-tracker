First part
explain what we are going to do
AI tensorflow wrap
with JS api/models

Giving us face recognition

First step live code

get Libary - javascript lib faceapi.min.js

get models - top level let them stay

Make HTML index page
<!doctype html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport"
          content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Webapp</title>
</head>
<body>

</body>
</html>

attach lib
<script src="/lib/face.js"></script>
attach script that I am making
<script src="/script.js"></script>

add video element
<video id="webcamVideo" width="1024" height="768" autoplay muted></video>

add a little style to center
<style>
    body {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
        width: 100vw;
        height: 100vh;
        display: flex;
        justify-content: center;
        align-items: center;
    }
</style>

enter into script
<script>
    //STEP ZERO
    const webcamVideo = document.getElementById('webcamVideo');
</script>

enter begin with navagator explain that is uses a promise
src object to viedeo
error checking
<script>
    //STEP ONE
    function getVideo () {
        navigator.mediaDevices.getUserMedia({ video: {} })
            .then(function(stream) {
                webcamVideo.srcObject = stream;
            })
            .catch(function(error) {
                console.error(error);
            });
</script>

then add event listener;
<script>
    //STEP TWO
    webcamVideo.addEventListener('play', function(){
        console.log("I am playing!");});
</script>

AI intergration or console first to see what is coming  run an array of models
<script>
    Promise.all([
        faceapi.nets.tinyFaceDetector.loadFromUri("./"),
        faceapi.nets.faceLandmark68Net.loadFromUri("./"),
        faceapi.nets.faceRecognitionNet.loadFromUri("./"),
        faceapi.nets.faceExpressionNet.loadFromUri("./")
    ]).then(getVideo);


    console.log(faceapi.nets);
</script>

set interval function and recieve detection data to get face detection
using the tingface detectoroptions

<script>
    //STEP FOUR
    setInterval(async function () {
        const detections = await faceapi.detectAllFaces(webcamVideo,
            new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceExpressions();
        console.log(detections);
    }, 100)
</script>

now make new canvas to stick it on top

<style>
    canvas {
        position: absolute;
    }
</style>
<script>

    webcamVideo.addEventListener('play', function(){
        console.log("I am playing!");

       //HERE FIRST ADD use the .createCanvasFormMedia method
        const canvas = faceapi.createCanvasFromMedia(webcamVideo);
        document.body.append(canvas);

        //SECOND TO PLACE OVER VIDEO
        const displaySize = { width: webcamVideo.width, height: webcamVideo.height };

        //ADD TO THE LAST
        faceapi.matchDimensions(canvas, displaySize);

        setInterval(async function () {
            const detections = await faceapi.detectAllFaces(webcamVideo,
                new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceExpressions();


            //THIRD ITEM TO ADD TO PROPERLY SIZE EVERYTHING use method to make it better
            const resizeDetections = faceapi.resizeResults(detections, displaySize);
            canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height);
            faceapi.draw.drawDetections(canvas, resizeDetections);

            //FINALLY ADD THE REST OF LANDMARKS AND EXPRESSION
            faceapi.draw.drawFaceLandmarks(canvas, resizeDetections);
            faceapi.draw.drawFaceExpressions(canvas, resizeDetections);

            console.log(detections);
        }, 100)
    });
</script>




